{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_device': 'cuda',\n",
      " 'eval': False,\n",
      " 'images': 'images',\n",
      " 'model_path': '',\n",
      " 'resolution': -1,\n",
      " 'sh_degree': 3,\n",
      " 'source_path': '/home/hjp/KUAICV/NIPS2024_LOCAL/datasets/tandt/train',\n",
      " 'white_background': False}\n",
      "\n",
      "<class 'arguments.ModelParams'> <class 'arguments.GroupParams'>\n",
      "-1\n",
      "\n",
      "{'_images': 'images',\n",
      " '_model_path': '',\n",
      " '_resolution': -1,\n",
      " '_source_path': '',\n",
      " '_white_background': False,\n",
      " 'data_device': 'cuda',\n",
      " 'eval': False,\n",
      " 'sh_degree': 3}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from arguments import ModelParams, PipelineParams, OptimizationParams\n",
    "\n",
    "parser = ArgumentParser(description=\"Training script parameters\")\n",
    "lp = ModelParams(parser)\n",
    "op = OptimizationParams(parser)\n",
    "pp = PipelineParams(parser)\n",
    "parser.add_argument('--ip', type=str, default=\"127.0.0.1\")\n",
    "parser.add_argument('--port', type=int, default=6009)\n",
    "parser.add_argument('--debug_from', type=int, default=-1)\n",
    "parser.add_argument('--detect_anomaly', action='store_true', default=False)\n",
    "parser.add_argument(\"--test_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "parser.add_argument(\"--save_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "parser.add_argument(\"--quiet\", action=\"store_true\")\n",
    "parser.add_argument(\"--checkpoint_iterations\", nargs=\"+\", type=int, default=[])\n",
    "parser.add_argument(\"--start_checkpoint\", type=str, default = None)\n",
    "\n",
    "args = parser.parse_args(args=[\n",
    "    \"--source_path\", \"/home/hjp/KUAICV/NIPS2024_LOCAL/datasets/tandt/train/\",\n",
    "])\n",
    "pprint(vars(lp.extract(args)))\n",
    "\n",
    "print()\n",
    "print(type(lp), type(lp.extract(args)))\n",
    "print(args.resolution)\n",
    "print()\n",
    "\n",
    "pprint(vars(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    active_sh_degree,\n",
    "    _xyz,\n",
    "    _features_dc,\n",
    "    _features_rest,\n",
    "    _scaling,\n",
    "    _rotation,\n",
    "    _opacity,\n",
    "    max_radii2D,\n",
    "    xyz_gradient_accum,\n",
    "    denom,\n",
    "    state_dict,\n",
    "    spatial_lr_scale,\n",
    "), saved_iter = torch.load(\n",
    "    \"/home/hjp/KUAICV/NIPS2024_LOCAL/gaussian-splatting-original/output/22ef02fc-3/chkpnt10.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_features_rest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read camera extrinsics\n",
    "import os\n",
    "\n",
    "from scene.dataset_readers import readCamerasFromTransforms\n",
    "from scene.colmap_loader import (\n",
    "    read_extrinsics_binary,\n",
    "    read_intrinsics_binary,\n",
    ")\n",
    "\n",
    "SRC_PATH = \"/home/hjp/KUAICV/NIPS2024_LOCAL/datasets/tandt/train\"\n",
    "cameras_extrinsic_file = os.path.join(SRC_PATH, \"sparse/0\", \"images.bin\")\n",
    "cameras_intrinsic_file = os.path.join(SRC_PATH, \"sparse/0\", \"cameras.bin\")\n",
    "cam_extrinsics = read_extrinsics_binary(cameras_extrinsic_file)\n",
    "cam_intrinsics = read_intrinsics_binary(cameras_intrinsic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11661, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_extrinsics[1].xys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import struct\n",
    "\n",
    "CameraModel = collections.namedtuple(\n",
    "    \"CameraModel\", [\"model_id\", \"model_name\", \"num_params\"])\n",
    "Camera = collections.namedtuple(\n",
    "    \"Camera\", \n",
    "    [\"id\", \"model\", \"width\", \"height\", \"params\"]\n",
    ")\n",
    "BaseImage = collections.namedtuple(\n",
    "    \"Image\",\n",
    "    [\n",
    "        \"id\", \"qvec\", \"tvec\", \"camera_id\",\n",
    "        \"name\", \"xys\", \"point3D_ids\"\n",
    "    ]\n",
    ")\n",
    "Point3D = collections.namedtuple(\n",
    "    \"Point3D\",\n",
    "    [\n",
    "        \"id\", \"xyz\", \"rgb\", \"error\",\n",
    "        \"image_ids\", \"point2D_idxs\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "CAMERA_MODELS = {\n",
    "    CameraModel(model_id=0, model_name=\"SIMPLE_PINHOLE\", num_params=3),\n",
    "    CameraModel(model_id=1, model_name=\"PINHOLE\", num_params=4),\n",
    "    CameraModel(model_id=2, model_name=\"SIMPLE_RADIAL\", num_params=4),\n",
    "    CameraModel(model_id=3, model_name=\"RADIAL\", num_params=5),\n",
    "    CameraModel(model_id=4, model_name=\"OPENCV\", num_params=8),\n",
    "    CameraModel(model_id=5, model_name=\"OPENCV_FISHEYE\", num_params=8),\n",
    "    CameraModel(model_id=6, model_name=\"FULL_OPENCV\", num_params=12),\n",
    "    CameraModel(model_id=7, model_name=\"FOV\", num_params=5),\n",
    "    CameraModel(model_id=8, model_name=\"SIMPLE_RADIAL_FISHEYE\", num_params=4),\n",
    "    CameraModel(model_id=9, model_name=\"RADIAL_FISHEYE\", num_params=5),\n",
    "    CameraModel(model_id=10, model_name=\"THIN_PRISM_FISHEYE\", num_params=12)\n",
    "}\n",
    "CAMERA_MODEL_IDS = dict([(camera_model.model_id, camera_model)\n",
    "                         for camera_model in CAMERA_MODELS])\n",
    "CAMERA_MODEL_NAMES = dict([(camera_model.model_name, camera_model)\n",
    "                           for camera_model in CAMERA_MODELS])\n",
    "\n",
    "\n",
    "class Image(BaseImage):\n",
    "    def qvec2rotmat(self):\n",
    "        return qvec2rotmat(self.qvec)\n",
    "    \n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "    return np.array([\n",
    "        [\n",
    "            1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
    "            2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "            2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]\n",
    "        ],\n",
    "        [\n",
    "            2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "            1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
    "            2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]\n",
    "        ],\n",
    "        [\n",
    "            2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "            2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "            1 - 2 * qvec[1]**2 - 2 * qvec[2]**2\n",
    "        ]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Could not recognize scene type!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1715457/3842208103.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mgaussian_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh_degree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mscene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gaussian-splatting/scene/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, gaussians, load_iteration, shuffle, resolution_scales)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mscene_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msceneLoadTypeCallbacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Blender\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhite_background\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Could not recognize scene type!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Could not recognize scene type!"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from arguments import ModelParams, PipelineParams, OptimizationParams\n",
    "\n",
    "# test GaussianModel\n",
    "from scene.gaussian_model import GaussianModel\n",
    "from scene import Scene\n",
    "\n",
    "parser = ArgumentParser(description=\"Training script parameters\")\n",
    "lp = ModelParams(parser)\n",
    "op = OptimizationParams(parser)\n",
    "pp = PipelineParams(parser)\n",
    "parser.add_argument('--ip', type=str, default=\"127.0.0.1\")\n",
    "parser.add_argument('--port', type=int, default=6009)\n",
    "parser.add_argument('--debug_from', type=int, default=-1)\n",
    "parser.add_argument('--detect_anomaly', action='store_true', default=False)\n",
    "parser.add_argument(\"--test_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "parser.add_argument(\"--save_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "parser.add_argument(\"--quiet\", action=\"store_true\")\n",
    "parser.add_argument(\"--checkpoint_iterations\", nargs=\"+\", type=int, default=[])\n",
    "parser.add_argument(\"--start_checkpoint\", type=str, default = None)\n",
    "args = parser.parse_args(args=[\n",
    "    \"--source_path\", \"/home/hjp/KUAICV/NIPS2024_LOCAL/datasets/tandt/train/\",\n",
    "])\n",
    "\n",
    "model_args = lp.extract(args)\n",
    "\n",
    "gaussian_model = GaussianModel(sh_degree=100)\n",
    "scene = Scene(model_args, gaussian_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjp/.conda/envs/gs/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading camera 301/301\n",
      "Loading Training Cameras\n",
      "Loading Test Cameras\n",
      "Number of points at initialisation :  182686\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.83 GiB (GPU 0; 23.69 GiB total capacity; 22.64 GiB already allocated; 47.81 MiB free; 22.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_8337/3842208103.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     32\u001b[0m \u001b[0mgaussian_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh_degree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 33\u001b[0;31m \u001b[0mscene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m~/KUAICV/NIPS2024_LOCAL/gaussian-splatting/scene/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, gaussians, load_iteration, shuffle, resolution_scales)\u001b[0m\n",
      "\u001b[1;32m     91\u001b[0m             ))\n",
      "\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussians\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_from_pcd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_cloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcameras_extent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/KUAICV/NIPS2024_LOCAL/gaussian-splatting/scene/gaussian_model.py\u001b[0m in \u001b[0;36mcreate_from_pcd\u001b[0;34m(self, pcd, spatial_lr_scale)\u001b[0m\n",
      "\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xyz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused_point_cloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features_dc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.83 GiB (GPU 0; 23.69 GiB total capacity; 22.64 GiB already allocated; 47.81 MiB free; 22.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from arguments import ModelParams, PipelineParams, OptimizationParams\n",
    "\n",
    "# test GaussianModel\n",
    "from scene.gaussian_model import GaussianModel\n",
    "from scene import Scene\n",
    "\n",
    "parser = ArgumentParser(description=\"Training script parameters\")\n",
    "lp = ModelParams(parser)\n",
    "op = OptimizationParams(parser)\n",
    "pp = PipelineParams(parser)\n",
    "parser.add_argument('--ip', type=str, default=\"127.0.0.1\")\n",
    "parser.add_argument('--port', type=int, default=6009)\n",
    "parser.add_argument('--debug_from', type=int, default=-1)\n",
    "parser.add_argument('--detect_anomaly', action='store_true', default=False)\n",
    "parser.add_argument(\"--test_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "parser.add_argument(\"--save_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "parser.add_argument(\"--quiet\", action=\"store_true\")\n",
    "parser.add_argument(\"--checkpoint_iterations\", nargs=\"+\", type=int, default=[])\n",
    "parser.add_argument(\"--start_checkpoint\", type=str, default = None)\n",
    "args = parser.parse_args(args=[\n",
    "    \"--source_path\", \"/home/hjp/KUAICV/NIPS2024_LOCAL/datasets/tandt/train/\",\n",
    "])\n",
    "\n",
    "model_args = lp.extract(args)\n",
    "\n",
    "gaussian_model = GaussianModel(sh_degree=100)\n",
    "scene = Scene(model_args, gaussian_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
